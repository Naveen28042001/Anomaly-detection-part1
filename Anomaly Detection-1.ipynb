{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac38fa2d-3b6f-46ad-9743-a929e8db7e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is anomaly detection and what is its purpose?\n",
    "\n",
    "Anomaly detection is a technique used in data mining and machine learning to identify patterns or instances that deviate significantly from the norm or the expected behavior within a dataset. The purpose of anomaly detection is to identify rare and unusual events, observations, or patterns that do not conform to the expected behavior, thereby signaling potential issues, errors, or abnormalities in the data.\n",
    "\n",
    "Anomaly detection is applied in various fields and industries for different purposes, including:\n",
    "Fraud Detection: \n",
    "    Identifying fraudulent activities, transactions, or behaviors in financial transactions, cybersecurity, or insurance claims.\n",
    "\n",
    "Network Security: \n",
    "    Detecting abnormal network traffic patterns or malicious activities that may indicate security breaches or cyber attacks.\n",
    "\n",
    "Healthcare Monitoring: \n",
    "    Identifying unusual patterns in patient data or medical records that may indicate the presence of diseases, abnormalities, or critical health conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e6d02-8e5e-4cc4-81aa-027dbcb0f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What are the key challenges in anomaly detection?\n",
    "\n",
    " Some of the key challenges in anomaly detection include:\n",
    "Imbalanced Data: \n",
    "    Anomalies are often rare compared to normal data points, leading to imbalanced datasets that can bias the detection model toward the majority class.\n",
    "\n",
    "Complex and Evolving Anomalies: \n",
    "    Anomalies can manifest in various complex forms, making it challenging to define a comprehensive set of rules or patterns to capture all possible anomalies, especially when anomalies evolve or adapt over time.\n",
    "\n",
    "Noisy Data: \n",
    "    Datasets may contain noise, outliers, or irregularities that are not necessarily indicative of true anomalies, leading to increased false positives and decreased detection accuracy.\n",
    "\n",
    "Scalability: \n",
    "    Analyzing large-scale datasets in real-time or near real-time can be computationally intensive, necessitating efficient algorithms and scalable techniques to handle big data and high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e12b6b1-2ab8-4f42-be5f-adff98c718dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n",
    "\n",
    "Unsupervised anomaly detection and supervised anomaly detection are two different approaches to identifying anomalies within a dataset, each with distinct characteristics and applications. Here's how they differ:\n",
    "\n",
    "Label Availability:\n",
    "    In unsupervised anomaly detection, the algorithm works with unlabeled data and does not rely on predefined labels or classes, whereas supervised anomaly detection requires labeled data, where anomalies are explicitly marked or identified during the training phase.\n",
    "\n",
    "Model Training:\n",
    "    Unsupervised anomaly detection algorithms learn the inherent structure of the data without explicit guidance, aiming to identify deviations from the norm or regular patterns. On the other hand, supervised anomaly detection algorithms train on labeled data, learning to distinguish between normal and anomalous instances based on the provided labels.\n",
    "\n",
    "Anomaly Definition:\n",
    "    Unsupervised anomaly detection does not require a predefined definition of anomalies, as it focuses on identifying patterns that deviate significantly from the norm. In contrast, supervised anomaly detection relies on predefined labels that explicitly specify the characteristics of anomalies.\n",
    "\n",
    "Scalability and Generalization: \n",
    "    Unsupervised anomaly detection methods are often more scalable and applicable to a wider range of datasets, as they do not depend on labeled data and can generalize to detect unknown anomalies. Supervised anomaly detection, while effective for labeled datasets, may struggle to generalize to new or unseen anomalies that were not part of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7cadbb-f56d-49ca-859f-927ea4afde36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are the main categories of anomaly detection algorithms?\n",
    "\n",
    "Anomaly detection algorithms can be broadly categorized into several main categories, each with its own unique approach to identifying anomalies within datasets. \n",
    "\n",
    "These categories include:\n",
    "\n",
    "Statistical Methods: \n",
    "    Statistical anomaly detection algorithms use statistical models to identify data points that significantly deviate from the expected behavior or distribution. These methods often include techniques such as Gaussian distribution models, Z-score analysis, and hypothesis testing.\n",
    "\n",
    "Machine Learning-Based Methods: \n",
    "    Machine learning-based anomaly detection algorithms leverage various machine learning techniques, such as clustering, classification, and regression, to identify anomalies within the data. These methods include k-nearest neighbors (KNN), support vector machines (SVM), and isolation forest algorithms.\n",
    "\n",
    "Proximity-Based Methods: \n",
    "    Proximity-based anomaly detection algorithms identify anomalies based on the proximity or density of data points within the dataset. These methods include techniques such as k-means clustering, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), and LOF (Local Outlier Factor) algorithms.\n",
    "\n",
    "Information-Theoretic Methods: \n",
    "    Information-theoretic anomaly detection algorithms assess the information content or entropy of data points to detect anomalies. These methods often involve measuring the complexity, randomness, or information content of data points and identifying deviations from expected information patterns.\n",
    "\n",
    "Deep Learning-Based Methods: \n",
    "    Deep learning-based anomaly detection algorithms utilize deep neural networks and deep learning architectures to automatically learn complex patterns and representations within the data. These methods include autoencoders, variational autoencoders (VAEs), and generative adversarial networks (GANs) for anomaly detection tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e763f4-18fe-4273-9e05-29ddd73d4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What are the main assumptions made by distance-based anomaly detection methods?\n",
    "\n",
    "Some of the main assumptions include:\n",
    "Assumption of Normality: \n",
    "    Distance-based methods often assume that the majority of the data points follow a normal or expected distribution, with anomalies representing deviations from this normal behavior.\n",
    "\n",
    "Euclidean Distance Metric: \n",
    "    Many distance-based anomaly detection methods rely on the Euclidean distance metric to measure the similarity or dissimilarity between data points. This assumes that the data can be adequately represented and compared using a Euclidean distance measure.\n",
    "\n",
    "Assumption of Linearity: \n",
    "    Some distance-based methods assume that the data can be effectively represented and analyzed using linear relationships, especially when using distance-based clustering techniques or linear models to identify anomalies.\n",
    "\n",
    "Assumption of Local Density: \n",
    "    Certain distance-based methods, such as density-based clustering algorithms, assume that anomalies are data points with significantly lower local density compared to their neighboring points. This assumption underlies the identification of outliers based on the local density of data points.\n",
    "\n",
    "Assumption of Distance Thresholds: \n",
    "    Some distance-based methods rely on predefined distance thresholds or distance-based clustering criteria to identify anomalies. These methods assume that anomalies can be distinguished based on specific distance thresholds or cutoff values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ae7a82-6a3f-4356-aefa-981921333606",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How does the LOF algorithm compute anomaly scores?\n",
    "\n",
    "\n",
    "The Local Outlier Factor (LOF) algorithm computes anomaly scores based on the concept of local density deviation, which measures the degree to which a data point deviates from the density of its neighbors. The algorithm identifies anomalies by assessing the local density of data points and comparing the density of each point to that of its neighbors. \n",
    "\n",
    "The steps involved in computing anomaly scores using the LOF algorithm are as follows:\n",
    "Calculate Local Reachability Density (LRD): \n",
    "    For each data point, the LRD is computed based on the average reachability distance of its k-nearest neighbors. The reachability distance is the maximum of the distance to the k-th nearest neighbor and the distance between the point and its k-th nearest neighbor. LRD measures how well-connected a point is to its local neighborhood.\n",
    "\n",
    "Calculate Local Reachability Distance (Lrd): \n",
    "    The Lrd is computed for each data point by taking the inverse of the LRD. This step is performed to obtain a measure that represents the local reachability distance of the point relative to the density of its neighbors.\n",
    "\n",
    "Compute Local Outlier Factor (LOF): \n",
    "    The LOF is calculated for each data point by comparing the Lrd of the point to that of its neighbors. The LOF represents the degree to which a point deviates from the local density of its neighbors. It is computed as the average ratio of the Lrd of the point to the Lrd of its k-nearest neighbors, with a value greater than 1 indicating that the point is in a less dense region compared to its neighbors.\n",
    "\n",
    "Assign Anomaly Scores: \n",
    "    Anomaly scores are assigned to each data point based on the LOF values, with higher values indicating that the point is more likely to be an outlier or anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e3e00-842a-4922-85ec-bdf93eaa5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What are the key parameters of the Isolation Forest algorithm?\n",
    "\n",
    "The Isolation Forest algorithm, a popular method for anomaly detection, relies on several key parameters that influence its performance and the identification of anomalies within a dataset. \n",
    "\n",
    "The main parameters of the Isolation Forest algorithm include:\n",
    "n_estimators: \n",
    "    This parameter refers to the number of isolation trees to be built during the training process. Increasing the number of trees can improve the accuracy of the algorithm but may also lead to longer computation times.\n",
    "\n",
    "max_samples: \n",
    "    The max_samples parameter determines the number of samples to be used for building each isolation tree. Setting a lower value for max_samples can accelerate the training process but may also decrease the effectiveness of the algorithm, while a higher value can enhance the robustness of the algorithm but may increase computational overhead.\n",
    "\n",
    "contamination: \n",
    "    The contamination parameter specifies the expected proportion of anomalies or outliers in the dataset. It helps in defining the threshold for identifying anomalies based on their isolation scores.\n",
    "\n",
    "max_features: \n",
    "    The max_features parameter controls the number of features to consider when splitting a node during the construction of each isolation tree. Setting a lower value can simplify the splitting process but may reduce the effectiveness of the algorithm, while a higher value may increase computational complexity without substantial performance improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788c19a1-1797-4e6e-adfb-8027945c71e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
    "using KNN with K=10?\n",
    "\n",
    "To compute the anomaly score using the K-Nearest Neighbors (KNN) algorithm, you need to consider the distance of the data point to its  K nearest neighbors. \n",
    "The anomaly score can be calculated as the average distance between the data point and its K nearest neighbors.\n",
    "\n",
    "Given that the data point has only 2 neighbors of the same class within a radius of 0.5 and we are using K=10, it implies that the data point has 2 neighbors within the specified radius.\n",
    "\n",
    "The anomaly score can be computed as the average distance between the data point and its 10 nearest neighbors. However, since there are only 2 neighbors within the radius, the anomaly score for the data point would be the average of the distances to these 2 neighbors.\n",
    "\n",
    "If the distances to the 2 neighbors within the radius of 0.5 are d1 and d2 the anomaly score can be calculated as:\n",
    "AnomalyScore=(d1+d2)/2\n",
    "This represents the average distance between the data point and its 2 nearest neighbors, considering the distances within the specified radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fb77ea-009a-4cc9-a70f-5a847831d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\n",
    "anomaly score for a data point that has an average path length of 5.0 compared to the average path\n",
    "length of the trees?\n",
    "\n",
    "Anomaly score = 2^(-E(h(n))/c(n))\n",
    "h(n) - average path length of the data point across all trees.e\n",
    "c(n) - average path length of unsuccessful search in a binary tree\n",
    "c(n) = 2*(log(n-1)+0.577)-(2*(n-1)/n)\n",
    "n - no. of datapoints\n",
    "c(n) = 2*(log(3000-1)+0.577)-(2*(3000-1)/3000)\n",
    "c(n) = 6.21\n",
    "E(h(n)) = 5.0\n",
    "Anomaly score = 2^((-5/6.21))\n",
    "Anomaly score = o.57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19e12c8-9f4d-48d4-98fc-68aa150f788d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
